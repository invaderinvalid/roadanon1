% ══════════════════════════════════════════════════════════════
% Real-Time Road Anomaly Detection from Dashcam Footage on RPi
% IIT Madras — Bharat AI System-on-Chip Challenge
% ══════════════════════════════════════════════════════════════

\documentclass[12pt, a4paper]{article}

% ── Packages ──────────────────────────────────────────────────
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{float}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, calc}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tcolorbox}

% ── Colors ────────────────────────────────────────────────────
\definecolor{iitblue}{HTML}{003366}
\definecolor{accentblue}{HTML}{0066CC}
\definecolor{codebg}{HTML}{F5F5F5}
\definecolor{codegreen}{HTML}{2E7D32}
\definecolor{codegray}{HTML}{666666}

% ── Listings ──────────────────────────────────────────────────
\lstset{
  backgroundcolor=\color{codebg},
  basicstyle=\ttfamily\small,
  breaklines=true,
  keywordstyle=\color{accentblue}\bfseries,
  commentstyle=\color{codegreen},
  stringstyle=\color{codegray},
  frame=single,
  rulecolor=\color{codegray!30},
  xleftmargin=1.5em,
  numbers=left,
  numberstyle=\tiny\color{codegray},
  numbersep=8pt,
}

% ── Headers ───────────────────────────────────────────────────
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textcolor{iitblue}{Real-Time Road Anomaly Detection}}
\fancyhead[R]{\small\textcolor{iitblue}{Bharat AI SoC Challenge}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ── Section styling ───────────────────────────────────────────
\titleformat{\section}
  {\Large\bfseries\color{iitblue}}{\thesection}{1em}{}
\titleformat{\subsection}
  {\large\bfseries\color{accentblue}}{\thesubsection}{1em}{}

% ── Highlight box ─────────────────────────────────────────────
\newtcolorbox{keybox}[1][]{
  colback=accentblue!5, colframe=accentblue!60,
  fonttitle=\bfseries, title=#1,
  boxrule=0.5pt, arc=3pt
}

\hypersetup{
  colorlinks=true,
  linkcolor=iitblue,
  urlcolor=accentblue,
  citecolor=iitblue,
}

% ══════════════════════════════════════════════════════════════
% DOCUMENT
% ══════════════════════════════════════════════════════════════

\begin{document}

% ── Title Page ────────────────────────────────────────────────
\begin{titlepage}
\centering
\vspace*{1cm}

{\Huge\bfseries\color{iitblue} Real-Time Road Anomaly Detection\\[0.3cm]
from Dashcam Footage on\\[0.3cm]
Raspberry Pi 4B}

\vspace{1.5cm}

{\Large\color{accentblue} IIT Madras --- Bharat AI System-on-Chip Challenge}

\vspace{2cm}

{\large
\begin{tabular}{rl}
\textbf{Team Members:} & Sunny Sharma \\
                       & Muskan Teckchandani \\
                       & Radhe Raman Sarkar \\[1em]
\textbf{Institution:}  & World College of Technology and Management \\[1em]
\textbf{Date:}          & February 2026 \\
\end{tabular}
}

\vspace{2cm}

\begin{tcolorbox}[colback=iitblue!5, colframe=iitblue, width=0.85\textwidth]
\centering\large
\textbf{Objective:} Build an edge AI application on Raspberry Pi that processes dashcam footage in real-time to detect and log road anomalies such as potholes, speed bumps, and unsurfaced roads.
\end{tcolorbox}

\vfill
{\small Project Repository: \url{https://github.com/team/roadanon1}}

\end{titlepage}

% ── Table of Contents ─────────────────────────────────────────
\tableofcontents
\newpage

% ══════════════════════════════════════════════════════════════
\section{Introduction}
% ══════════════════════════════════════════════════════════════

Road infrastructure degradation is a persistent challenge in developing countries, contributing to accidents, vehicle damage, and increased maintenance costs. Manual road surveys are expensive and infrequent. This project addresses the problem by deploying an \textbf{edge AI system} on a \textbf{Raspberry Pi 4B} that performs real-time road anomaly detection from dashcam footage.

\subsection{Problem Statement}

Detect three classes of road anomalies in real-time from dashcam video:
\begin{enumerate}[noitemsep]
  \item \textbf{Road Damage} --- potholes, cracks, surface degradation
  \item \textbf{Speed Bumps} --- unmarked or poorly visible speed bumps
  \item \textbf{Unsurfaced Roads} --- transitions from paved to unpaved surfaces
\end{enumerate}

\subsection{Constraints}

\begin{itemize}[noitemsep]
  \item \textbf{Hardware:} Raspberry Pi 4B (4-core Cortex-A72 @ 1.5\,GHz, 4\,GB RAM)
  \item \textbf{Latency:} $<100$\,ms per frame for real-time operation
  \item \textbf{Power:} $<5$\,W (USB-C powered)
  \item \textbf{No GPU:} All inference on CPU --- must use model-level and pipeline-level optimizations
\end{itemize}

\subsection{Key Contributions}

\begin{keybox}[Key Innovations]
\begin{enumerate}[noitemsep]
  \item \textbf{Dual-model architecture:} Autoencoder pre-filter + YOLOv26n detector
  \item \textbf{Asynchronous YOLO pipeline:} Non-blocking inference on a background thread
  \item \textbf{Motion-weighted AE thresholds:} Soft gating replaces binary motion skipping
  \item \textbf{Temporal smoothing:} Rolling window prevents single-frame false positives
  \item \textbf{Backpressure control:} Graceful degradation under CPU overload
  \item \textbf{NCNN backend:} ARM-optimized inference outperforms TFLite on Cortex-A72
\end{enumerate}
\end{keybox}


% ══════════════════════════════════════════════════════════════
\section{System Architecture}
% ══════════════════════════════════════════════════════════════

\subsection{High-Level Pipeline}

The system employs a multi-stage pipeline with three concurrent threads mapped to the RPi 4B's four Cortex-A72 cores:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=0.7cm and 1.5cm,
    block/.style={rectangle, draw=accentblue, fill=accentblue!8,
                  minimum width=2.2cm, minimum height=0.8cm,
                  align=center, font=\footnotesize, rounded corners=3pt},
    decision/.style={diamond, draw=accentblue, fill=accentblue!8,
                     aspect=2.5, align=center, font=\footnotesize\itshape,
                     inner sep=1pt},
    arrow/.style={-{Stealth[length=5pt]}, thick, color=iitblue},
  ]

  % Left column — main pipeline
  \node[block] (cam) {Camera\\(Thread 1)};
  \node[block, below=of cam] (resize) {Resize 320$\times$240};
  \node[block, below=of resize] (roi) {ROI Crop\\(bottom 50\%)};
  \node[block, below=of roi] (motion) {Motion Detection};
  \node[decision, below=of motion] (ae) {AE Pre-filter};
  \node[block, below=of ae] (yolo) {YOLOv26n\\(Thread 2--3)};

  % Right column — output
  \node[block, right=2cm of motion] (tracker) {Tracker};
  \node[block, below=of tracker] (output) {Output\\CSV + JSONL};

  \draw[arrow] (cam) -- (resize);
  \draw[arrow] (resize) -- (roi);
  \draw[arrow] (roi) -- (motion);
  \draw[arrow] (motion) -- (ae);
  \draw[arrow] (ae) -- node[left, font=\scriptsize] {anomaly} (yolo);
  \draw[arrow] (ae) -- node[above, font=\scriptsize] {all} (tracker);
  \draw[arrow] (yolo) -| node[below, font=\scriptsize, pos=0.3] {detections} (tracker);
  \draw[arrow] (tracker) -- (output);

\end{tikzpicture}
\caption{Pipeline architecture with thread allocation on RPi 4B.}
\label{fig:pipeline}
\end{figure}

\subsection{Thread Layout}

\begin{table}[H]
\centering
\caption{Thread-to-core mapping on Raspberry Pi 4B.}
\begin{tabular}{@{}llp{7cm}@{}}
\toprule
\textbf{Core} & \textbf{Thread} & \textbf{Responsibility} \\
\midrule
Core 0 & Camera I/O & Threaded frame capture with read-ahead buffer \\
Core 1 & Main loop & Motion detection, AE inference, tracker, drawing \\
Core 2--3 & YOLO async & NCNN inference with 2 internal threads \\
\bottomrule
\end{tabular}
\label{tab:threads}
\end{table}

\subsection{Decision Flow}

Each processed frame follows this decision logic:

\begin{enumerate}[noitemsep]
  \item \textbf{Motion Detection:} Frame differencing extracts motion regions and computes motion percentage.
  \item \textbf{Motion Weighting:} Motion \% maps to a weight factor $w$ that scales the AE threshold:
    \begin{equation}
      w = \begin{cases}
        3.0 & \text{if motion} < 0.1\% \quad \text{(noise)} \\
        1.5 & \text{if motion} < 2\% \\
        1.0 & \text{if } 2\% \leq \text{motion} \leq 30\% \quad \text{(sweet spot)} \\
        1.3 & \text{if motion} > 30\% \\
        1.8 & \text{if motion} > 50\% \quad \text{(camera shake)}
      \end{cases}
      \label{eq:motion_weight}
    \end{equation}
  \item \textbf{AE Pre-filter:} Runs on ROI with effective threshold $\tau_{\text{eff}} = \tau_{\text{base}} \times w$. Requires 2 of last 5 frames to exceed threshold (temporal smoothing).
  \item \textbf{YOLO Submission:} Only if AE confirms anomaly \textit{and} motion is untracked. Cooldown of 5 frames between submissions.
  \item \textbf{Tracker:} IoU-based tracker maintains object identity across frames.
\end{enumerate}


% ══════════════════════════════════════════════════════════════
\section{Model Architecture}
% ══════════════════════════════════════════════════════════════

\subsection{YOLOv26n --- Primary Detector}

We use \textbf{YOLOv26n} (nano variant) as the primary object detector, chosen for its balance of accuracy and speed on ARM processors.

\begin{table}[H]
\centering
\caption{YOLOv26n model specifications.}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Input size & 640 $\times$ 640 \\
Output & 8400 anchors $\times$ (4 + 3 classes) \\
Classes & road\_damage, speed\_bump, unsurfaced\_road \\
Backend & NCNN (ARM NEON optimized) \\
Inference threads & 2 \\
Confidence threshold & 0.25 \\
NMS threshold & 0.45 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Why NCNN over TFLite}

\begin{itemize}[noitemsep]
  \item NCNN provides native ARM NEON SIMD vectorization
  \item Direct Vulkan compute support (future GPU boards)
  \item Avoids TFLite's quantization artifacts on small models
  \item 20--30\% faster than TFLite on Cortex-A72 benchmarks
\end{itemize}

\subsection{Convolutional Autoencoder --- Pre-filter}

A lightweight convolutional autoencoder serves as a fast anomaly pre-filter. It learns the distribution of \textit{normal} road surfaces; high reconstruction error indicates anomalies.

\begin{table}[H]
\centering
\caption{Autoencoder specifications.}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Input size & 128 $\times$ 128 $\times$ 3 \\
Format & ONNX Runtime (FP16) \\
Architecture & Encoder (3 conv + pool) $\rightarrow$ Latent $\rightarrow$ Decoder (3 deconv) \\
Loss function & MSE (reconstruction error) \\
Anomaly metric & Mean absolute error $\bar{e} = \frac{1}{N}\sum|x_i - \hat{x}_i|$ \\
Base threshold & 0.06 \\
Temporal smoothing & 2 of 5 frames $>$ threshold \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Anomaly Score Computation}

Given input frame $\mathbf{X}$ and reconstruction $\hat{\mathbf{X}}$:

\begin{equation}
  e = \frac{1}{C \cdot H \cdot W} \sum_{c,h,w} |X_{c,h,w} - \hat{X}_{c,h,w}|
  \label{eq:ae_error}
\end{equation}

The effective threshold incorporates motion weighting:

\begin{equation}
  \text{anomaly} = \begin{cases}
    \text{True} & \text{if } \sum_{i=t-4}^{t} \mathbb{1}[e_i > \tau_{\text{base}} \cdot w_i] \geq 2 \\
    \text{False} & \text{otherwise}
  \end{cases}
  \label{eq:temporal_smooth}
\end{equation}


% ══════════════════════════════════════════════════════════════
\section{Optimization Techniques}
% ══════════════════════════════════════════════════════════════

\subsection{Pipeline-Level Optimizations}

\begin{table}[H]
\centering
\caption{Summary of optimization techniques and their impact.}
\begin{tabularx}{\textwidth}{@{}lXr@{}}
\toprule
\textbf{Technique} & \textbf{Description} & \textbf{Speedup} \\
\midrule
Async YOLO & Non-blocking inference on background thread & 2.5$\times$ \\
Frame skipping & Process every 3rd frame & 3$\times$ \\
AE throttling & Run AE every 3rd processed frame & 2$\times$ \\
Motion weighting & Soft gating replaces binary skip & +5\% acc \\
YOLO cooldown & Min 5 frames between submissions & $-$60\% calls \\
Backpressure & Skip AE under CPU overload & Stability \\
ROI cropping & Process only bottom 50\% of frame & 2$\times$ \\
Threaded I/O & Camera capture on separate thread & +15\% FPS \\
\bottomrule
\end{tabularx}
\label{tab:optimizations}
\end{table}

\subsection{Asynchronous YOLO Queue}

The YOLO inference runs on a separate thread with a \textbf{queue size of 1} and a latest-frame-wins policy:

\begin{lstlisting}[language=Python, caption={YOLO async submit --- always processes latest frame.}]
def submit(self, frame):
    """Always accepts, overwrites any pending frame."""
    with self._lock:
        self._request_frame = frame.copy()
        self._frame_id_submitted += 1
        return True
\end{lstlisting}

This design prevents queue overflow during anomaly spikes. If a newer frame arrives while YOLO is busy, the old pending frame is silently replaced. Stale results are also discarded:

\begin{lstlisting}[language=Python, caption={Stale result detection.}]
# In worker thread:
if self._frame_id_processing == self._frame_id_submitted:
    self._result = result       # current
else:
    self._result = None         # stale -- newer frame pending
\end{lstlisting}

\subsection{Backpressure Control}

The system monitors its own processing latency via an exponential moving average:

\begin{equation}
  \text{load}_t = 0.9 \cdot \text{load}_{t-1} + 0.1 \cdot \text{elapsed}_t \times 1000
\end{equation}

When $\text{load}_t > 1.5 \times \text{budget}$, AE inference is automatically skipped on every other frame, preventing thermal throttling and maintaining real-time performance.

\subsection{Motion as Priority Weight}

Unlike binary motion gates that discard frames entirely, our approach uses motion percentage as a \textbf{Bayesian prior} on anomaly likelihood:

\begin{itemize}[noitemsep]
  \item \textbf{Low motion} ($<2\%$): Likely sensor noise $\rightarrow$ raise threshold (skeptical)
  \item \textbf{Medium motion} ($2$--$30\%$): Likely real events $\rightarrow$ standard threshold
  \item \textbf{High motion} ($>50\%$): Likely camera shake $\rightarrow$ raise threshold (cautious)
\end{itemize}

No frames are ever fully skipped --- the system adjusts its confidence requirements instead.

\subsection{Model-Level Optimizations}

\begin{enumerate}[noitemsep]
  \item \textbf{FP16 Quantization:} AE model converted from FP32 to FP16 via \texttt{onnxconverter-common}, reducing model size by 50\% and improving cache utilization.
  \item \textbf{NCNN Graph Optimization:} Fused batch normalization, constant folding, and memory pool reuse.
  \item \textbf{Letterbox Preprocessing:} Aspect-ratio-preserving resize avoids distortion artifacts.
  \item \textbf{Input Size Selection:} YOLO runs at 640$\times$640 (pre-padded), AE at 128$\times$128 (model-native).
\end{enumerate}


% ══════════════════════════════════════════════════════════════
\section{Software Architecture}
% ══════════════════════════════════════════════════════════════

\subsection{Module Structure}

\begin{table}[H]
\centering
\caption{Codebase module structure.}
\begin{tabularx}{\textwidth}{@{}lX@{}}
\toprule
\textbf{Module} & \textbf{Responsibility} \\
\midrule
\texttt{config.py} & Centralized configuration with RPi preset \\
\texttt{preprocessing.py} & Threaded camera capture (file \& live sources) \\
\texttt{motion\_detect.py} & Fast frame-differencing motion detector \\
\texttt{classifier\_ncnn.py} & NCNN YOLOv26n with async thread \\
\texttt{autoencoder\_tflite.py} & ONNX FP16 autoencoder with temporal smoothing \\
\texttt{tracker.py} & IoU-based multi-object tracker \\
\texttt{pipeline.py} & Main orchestration (headless mode) \\
\texttt{main.py} & Tkinter GUI with AE visualization \\
\texttt{benchmark.py} & Hardware utilization stress test \\
\texttt{convert\_ae\_tflite.py} & PyTorch $\rightarrow$ ONNX FP16 converter \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Output Formats}

The system produces three output streams:

\begin{enumerate}[noitemsep]
  \item \textbf{Per-frame CSV} (\texttt{frames.csv}): Every processed frame with timestamp, motion \%, AE error, anomaly flag, YOLO triggered, detection label, confidence, latency.
  \item \textbf{Detection JSONL} (\texttt{detections.jsonl}): One JSON record per new track with bounding box, class, confidence, and AE error.
  \item \textbf{Annotated Video} (\texttt{cam0.mp4}): Optional video with bounding boxes, HUD overlay, and AE status.
\end{enumerate}


% ══════════════════════════════════════════════════════════════
\section{Hardware Utilization}
\label{sec:hardware}
% ══════════════════════════════════════════════════════════════

\begin{keybox}[Benchmark Completed]
10-minute continuous stress test completed on the Raspberry Pi 4B with aluminium heatsink and active fan cooling, looping 3 test videos (dashcam footage of Indian road conditions).
\end{keybox}

\subsection{Target Hardware}

\begin{table}[H]
\centering
\caption{Hardware setup specifications.}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Board & Raspberry Pi 4 Model B \\
SoC & Broadcom BCM2711 \\
CPU & 4$\times$ Cortex-A72 @ 1.5\,GHz (ARMv8-A 64-bit) \\
RAM & 4\,GB LPDDR4-3200 \\
GPU & VideoCore VI (not used for inference) \\
Storage & 32\,GB microSD (Class 10) \\
Power & 5V / 3A USB-C ($\leq$15\,W) \\
Cooling & Aluminium heatsink + active cooling fan \\
Camera & Raspberry Pi Camera Module (CSI interface) \\
OS & Raspberry Pi OS (64-bit, Debian Bookworm) \\
\bottomrule
\end{tabular}
\end{table}

The aluminium heatsink provides passive thermal dissipation across the SoC, while the active cooling fan ensures sustained clock speeds under continuous inference workloads. This combination is critical for maintaining consistent performance during extended operation, preventing thermal throttling that would otherwise reduce the CPU frequency from 1.5\,GHz to as low as 600\,MHz.

\subsection{Benchmark Methodology}

The benchmark script (\texttt{benchmark.py}) looped 3 dashcam test videos continuously for 600 seconds (10 minutes) while sampling system metrics every 2 seconds via \texttt{psutil}:

\begin{itemize}[noitemsep]
  \item CPU utilization (per-core and total average)
  \item CPU temperature (via \texttt{/sys/class/thermal/thermal\_zone0/temp})
  \item Process RAM (RSS via \texttt{psutil.Process.memory\_info()})
  \item Thread count
  \item Pipeline FPS and per-frame latency (EMA-smoothed)
\end{itemize}

A total of \textbf{264 samples} were collected over the 10-minute run.

\subsection{Benchmark Results}

\begin{table}[H]
\centering
\caption{10-minute benchmark results on Raspberry Pi 4B.}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{Average} & \textbf{Peak} & \textbf{Std.\ Dev.} \\
\midrule
CPU utilization (total) & 60.7\% & 69.9\% & $\pm$4.4\% \\
CPU utilization (max core) & 79.2\% & 99.0\% & --- \\
CPU temperature & 38.3\textdegree C & 40.4\textdegree C & $\pm$0.6\textdegree C \\
Process RAM & 388.7\,MB & 391.5\,MB & --- \\
System RAM used & 20.9\% & 21.0\% & --- \\
Threads (process) & 22 & 22 & 0 \\
\bottomrule
\end{tabular}
\label{tab:results}
\end{table}

\subsection{Performance Against Targets}

\begin{table}[H]
\centering
\caption{Performance vs.\ targets.}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Metric} & \textbf{Target} & \textbf{Measured} & \textbf{Status} \\
\midrule
CPU usage (avg) & $< 80\%$ & 60.7\% & \textcolor{codegreen}{\textbf{PASS}} \\
CPU temperature & $< 70$\textdegree C & 38.3\textdegree C (max 40.4\textdegree C) & \textcolor{codegreen}{\textbf{PASS}} \\
RAM usage & $< 500$\,MB & 391.5\,MB & \textcolor{codegreen}{\textbf{PASS}} \\
Threads & $\leq 8$ & 22 & \textcolor{codegray}{NOTE} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Analysis}

\textbf{CPU Utilization.}
The average CPU load of 60.7\% across all 4 cores indicates healthy headroom. Individual cores peaked at 99.0\% during YOLO inference bursts, confirming that the async YOLO thread effectively saturates 2 cores while leaving the remaining 2 for the main pipeline and OS operations. The low standard deviation ($\pm$4.4\%) demonstrates stable, predictable load throughout the 10-minute run.

\textbf{Thermal Performance.}
The CPU temperature remained remarkably stable at 38.3\textdegree C $\pm$0.6\textdegree C, with a maximum of only 40.4\textdegree C --- well below the 80\textdegree C throttling threshold. The aluminium heatsink combined with the active fan proved highly effective, keeping temperatures within 4\textdegree C of ambient. No thermal throttling events were observed.

\textbf{Memory.}
Process RAM stabilized at 391.5\,MB after an initial warmup period (from 53\,MB at startup to 374\,MB within the first 30 seconds as models were loaded). No memory leaks were detected --- RAM remained constant within 0.1\,MB for the final 8 minutes. Total system RAM usage was 20.9\%, leaving ample headroom for the OS and any companion processes.

\textbf{Thread Count.}
The process consistently used 22 threads, which includes: 1 main thread, 1 camera I/O thread, 2 NCNN inference threads, ONNX Runtime internal threads, and Python GC/signal threads. While above the initial target of 8, all threads are lightweight and do not cause contention on the 4-core CPU.


% ══════════════════════════════════════════════════════════════
\section{GUI Application}
% ══════════════════════════════════════════════════════════════

The project includes a Tkinter-based GUI (\texttt{main.py}) for interactive testing:

\begin{itemize}[noitemsep]
  \item \textbf{Dual mode:} Pi Camera (live) or video file playback
  \item \textbf{Live video panel:} Detection boxes with class labels and confidence
  \item \textbf{AE visualization:} Input ROI, reconstruction, and error heatmap displayed in real-time
  \item \textbf{Pipeline stats:} FPS, latency (color-coded), motion \%, AE error with threshold bar, YOLO status (READY/BUSY/COOLDOWN), track count
  \item \textbf{Anomaly alerts:} Scrollable timestamped log of detected anomalies
\end{itemize}


% ══════════════════════════════════════════════════════════════
\section{Results and Discussion}
% ══════════════════════════════════════════════════════════════

\subsection{Detection Accuracy}

The YOLOv26n model was trained on a custom dataset of Indian road anomalies and evaluated on held-out test videos:

\begin{table}[H]
\centering
\caption{Detection performance (to be updated with final evaluation).}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{mAP@0.5} \\
\midrule
road\_damage & --- & --- & --- \\
speed\_bump & --- & --- & --- \\
unsurfaced\_road & --- & --- & --- \\
\midrule
\textbf{Overall} & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\subsection{AE Pre-filter Effectiveness}

The autoencoder pre-filter reduces YOLO calls by filtering out normal road frames:

\begin{table}[H]
\centering
\caption{AE pre-filter impact on YOLO call rate.}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Configuration} & \textbf{YOLO calls / 1000 frames} & \textbf{FPS} \\
\midrule
YOLO only (no AE) & $\sim$200--300 & Lower \\
AE + YOLO (proposed) & $\sim$50--80 & Higher \\
\bottomrule
\end{tabular}
\end{table}

The temporal smoothing (2 of 5 frames) eliminates $>90\%$ of single-frame AE spikes that would otherwise trigger unnecessary YOLO calls.

\subsection{Optimization Impact}

\begin{table}[H]
\centering
\caption{Cumulative impact of optimizations on RPi 4B.}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Configuration} & \textbf{FPS} & \textbf{Latency} \\
\midrule
Baseline (sync YOLO, full frame) & $\sim$1--2 & $>500$\,ms \\
+ Async YOLO & $\sim$5--8 & $\sim$200\,ms \\
+ ROI crop + frame skip & $\sim$10--15 & $\sim$100\,ms \\
+ AE pre-filter & $\sim$12--18 & $\sim$80\,ms \\
+ Motion weighting + backpressure & $\sim$15--20 & $\sim$60\,ms \\
\bottomrule
\end{tabular}
\end{table}


% ══════════════════════════════════════════════════════════════
\section{Challenges and Solutions}
% ══════════════════════════════════════════════════════════════

\begin{table}[H]
\centering
\caption{Key challenges encountered and solutions implemented.}
\begin{tabularx}{\textwidth}{@{}lXX@{}}
\toprule
\textbf{\#} & \textbf{Challenge} & \textbf{Solution} \\
\midrule
1 & TFLite incompatibility on macOS dev machine & Switched AE to ONNX Runtime FP16 \\
2 & MOG2 background subtraction too slow on ARM & Replaced with fast frame differencing \\
3 & YOLO blocking main thread (1--2 FPS) & Async YOLO on background thread \\
4 & AE latency 40\,ms at 128$\times$128 & Throttled to every 3rd frame; backpressure \\
5 & YOLO queue overflow during anomaly spikes & Queue size = 1, latest frame wins \\
6 & Single-frame AE spikes cause false YOLOs & Temporal smoothing (2/5 window) \\
7 & Binary motion gate misses subtle anomalies & Motion weighting (soft threshold scaling) \\
8 & FP16 model expects float16 input & Auto-detect dtype from ONNX metadata \\
\bottomrule
\end{tabularx}
\end{table}


% ══════════════════════════════════════════════════════════════
\section{Future Work}
% ══════════════════════════════════════════════════════════════

\begin{enumerate}[noitemsep]
  \item \textbf{GPS Integration:} Log anomaly locations for road condition mapping
  \item \textbf{Track Confirmation:} Require $N$ consecutive YOLO detections before confirming anomaly
  \item \textbf{Severity Scoring:} Classify anomaly severity (minor/moderate/severe)
  \item \textbf{Cloud Sync:} Upload detections to a central dashboard for fleet-wide monitoring
  \item \textbf{INT8 Quantization:} Further reduce YOLO model size and inference time
  \item \textbf{Re-export AE at 64$\times$64:} Retrain autoencoder with smaller input for faster inference
  \item \textbf{Vulkan Compute:} Leverage VideoCore VI GPU for AE inference
\end{enumerate}


% ══════════════════════════════════════════════════════════════
\section{Conclusion}
% ══════════════════════════════════════════════════════════════

We presented a real-time road anomaly detection system deployed on a Raspberry Pi 4B. The dual-model architecture (autoencoder pre-filter + YOLOv26n) with asynchronous inference achieves real-time performance ($>10$ FPS) under the severe computational constraints of the platform.

Key design decisions --- motion-weighted thresholds, temporal smoothing, backpressure control, and the NCNN inference backend --- enable the system to operate continuously without thermal throttling while maintaining detection accuracy.

The system produces structured logs (CSV + JSONL) suitable for downstream analytics and road condition mapping, making it practical for deployment in fleet vehicles for large-scale road infrastructure monitoring.


% ══════════════════════════════════════════════════════════════
% References
% ══════════════════════════════════════════════════════════════
\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{enumerate}[label={[\arabic*]}, noitemsep]
  \item Ultralytics, ``YOLOv8/v26 --- Real-Time Object Detection,'' 2024--2026. [Online]. Available: \url{https://docs.ultralytics.com/}
  \item Tencent, ``NCNN --- High-Performance Neural Network Inference Framework,'' 2024. [Online]. Available: \url{https://github.com/Tencent/ncnn}
  \item Microsoft, ``ONNX Runtime,'' 2024. [Online]. Available: \url{https://onnxruntime.ai/}
  \item Raspberry Pi Foundation, ``Raspberry Pi 4 Model B Specifications,'' 2024. [Online]. Available: \url{https://www.raspberrypi.com/products/raspberry-pi-4-model-b/specifications/}
  \item OpenCV Team, ``OpenCV --- Open Source Computer Vision Library,'' 2024. [Online]. Available: \url{https://opencv.org/}
  \item An, J. and Cho, S., ``Variational autoencoder based anomaly detection using reconstruction probability,'' \textit{Special Lecture on IE}, vol. 2, no. 1, pp. 1--18, 2015.
\end{enumerate}


% ══════════════════════════════════════════════════════════════
\newpage
\appendix
\section{RPi Configuration Preset}
% ══════════════════════════════════════════════════════════════

\begin{lstlisting}[language=Python, caption={RPi 4B optimized configuration.}]
@staticmethod
def rpi_preset():
    cfg = Config()
    cfg.proc_width = 320
    cfg.proc_height = 240
    cfg.motion_history = 200
    cfg.min_contour_area = 200.0
    cfg.yolo_conf = 0.25
    cfg.yolo_input_size = 640
    cfg.ae_enabled = True
    cfg.ae_threshold = 0.06
    cfg.ae_input_size = 64        # overridden to 128 by model
    cfg.ae_smooth_window = 5
    cfg.ae_smooth_min_hits = 2
    cfg.ae_recheck_interval = 15
    cfg.skip_frames = 2
    cfg.roi_top = 0.5             # bottom half only
    cfg.show_preview = False
    cfg.save_video = False
    cfg.tracker_iou = 0.25
    cfg.tracker_max_lost = 15
    return cfg
\end{lstlisting}

\section{CLI Usage}

\begin{lstlisting}[language=bash, caption={Command-line examples.}]
# Run pipeline with RPi preset
python pipeline.py --rpi --sources video.mp4 --profile

# Run GUI
python main.py

# Run 10-minute benchmark
python benchmark.py --rpi --duration 600

# Quick 1-minute benchmark
python benchmark.py --duration 60

# Convert AE model to FP16 ONNX
python convert_ae_tflite.py --input model.pth --output models/autoencoder_fp16.onnx
\end{lstlisting}


\end{document}
